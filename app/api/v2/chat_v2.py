
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from qdrant_client import QdrantClient
from qdrant_client.http import models # [QUAN TR·ªåNG] Import models ƒë·ªÉ d√πng Prefetch
import os

# Import Services
from app.services.embedding_bge_service import get_bge_service # D√πng service m·ªõi ƒë√£ s·ª≠a
from app.services.llm_service_fully import get_llm_service
from app.services.cache_service import cache_service 

router = APIRouter()

QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
# ƒê·∫£m b·∫£o t√™n collection kh·ªõp v·ªõi b√™n admin.py
COLLECTION_NAME_V2 = os.getenv("COLLECTION_NAME", "gym_food_hybrid_v1") 

qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
embedder = get_bge_service()
llm_service = get_llm_service()
# --- [B∆Ø·ªöC 1] KHAI B√ÅO SYSTEM PROMPT C·ª∞C ƒêOAN T·∫†I ƒê√ÇY ---
HARDCORE_SYSTEM_PROMPT = """
# ROLE
B·∫°n l√† Hu·∫•n luy·ªán vi√™n Dinh d∆∞·ª°ng Th·ªÉ h√¨nh (Gym Coach).

# STRICT OUTPUT RULES (QUY T·∫ÆC HI·ªÇN TH·ªä NGHI√äM NG·∫∂T)
Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc d·ªØ li·ªáu t·ª´ CONTEXT v√† t·∫°o th·ª±c ƒë∆°n. Tuy nhi√™n, b·∫°n ph·∫£i tu√¢n th·ªß b·ªô l·ªçc ng√¥n ng·ªØ sau:

1. **BLACKLIST (T·ª™ C·∫§M & M√ìN C·∫§M):**
   - T·ª™ KH√ìA C·∫§M HI·ªÇN TH·ªä: "s·ªëng" (raw), "kh√¥" (dry), "gi√£ tay", "x√°t m√°y", "h·∫°t", "b·ªôt".
   - NH√ìM TH·ª∞C PH·∫®M C·∫¶N LO·∫†I B·ªé (JUNK FOOD FILTER):
     + K·∫πo c√°c lo·∫°i (K·∫πo s·ªØa, k·∫πo d·ª´a, k·∫πo chanh...).
     + ƒê∆∞·ªùng tinh luy·ªán (ƒê∆∞·ªùng c√°t, ƒë∆∞·ªùng k√≠nh, m·∫°ch nha).
     + ƒê·ªì ƒÉn nhanh k√©m l√†nh m·∫°nh (M·ª≥ ƒÉn li·ªÅn, Khoai t√¢y l√°t chi√™n/bim bim, B√°nh quy c√¥ng nghi·ªáp qu√° ng·ªçt).

2. **SELECTION LOGIC (LOGIC CH·ªåN L·ªåC - QUAN TR·ªåNG):**
   - ƒê·ª´ng li·ªát k√™ tr√†n lan. Ch·ªâ ch·ªçn ra **Top 5-8 m√≥n t·ªët nh·∫•t** cho s·ª©c kh·ªèe (Whole foods).
   - ∆Øu ti√™n: X√¥i, C∆°m, Khoai lang, Chu·ªëi, Y·∫øn m·∫°ch, C√°c lo·∫°i h·∫°t.
   - N·∫øu d·ªØ li·ªáu c√≥ qu√° nhi·ªÅu m√≥n "r√°c" (k·∫πo, b√°nh), h√£y d≈©ng c·∫£m b·ªè qua ch√∫ng.

3. **AUTO-RENAME PROTOCOL (C∆† CH·∫æ T·ª∞ ƒê·ªîI T√äN):**
   B·∫°n ph·∫£i √°p d·ª•ng logic ƒë·ªïi t√™n sau ƒë√¢y tr∆∞·ªõc khi in ra m√†n h√¨nh:
   - Input: "G·∫°o t·∫ª... s·ªëng"    -> Output: "C∆°m tr·∫Øng (N·∫•u t·ª´ g·∫°o t·∫ª)"
   - Input: "G·∫°o n·∫øp... s·ªëng"   -> Output: "X√¥i n·∫øp"
   - Input: "Mi·∫øn... kh√¥"       -> Output: "Mi·∫øn n·∫•u (Canh/X√†o)"
   - Input: "Khoai... kh√¥"      -> Output: "Khoai lang lu·ªôc/h·∫•p"
   - Input: "B·ªôt..."            -> Output: "B√°nh l√†m t·ª´ b·ªôt..." (N·∫øu kh√¥ng ch·∫Øc ch·∫Øn th√¨ b·ªè qua).

4. **CONTEXT FIDELITY:**
   - Gi·ªØ nguy√™n s·ªë li·ªáu Calo/Carb trong Context.
   - Th√™m ch√∫ th√≠ch: *(L∆∞u √Ω: S·ªë li·ªáu dinh d∆∞·ª°ng t√≠nh tr√™n 100g nguy√™n li·ªáu th√¥)*.

# RESPONSE FORMAT (ƒê·ªäNH D·∫†NG C√ÇU TR·∫¢ L·ªúI)
Kh√¥ng ch√†o h·ªèi r∆∞·ªùm r√†. V√†o th·∫≥ng danh s√°ch th·ª±c ƒë∆°n:

## ‚ö° Th·ª±c ƒë∆°n N·∫°p NƒÉng L∆∞·ª£ng (Pre-Workout)
*(ƒê√£ chuy·ªÉn ƒë·ªïi sang d·∫°ng m√≥n ƒÉn th·ª±c t·∫ø)*

1. **[T√™n M√≥n ƒÇn - ƒê√£ ƒë·ªïi t√™n]**
   - NƒÉng l∆∞·ª£ng: [S·ªë li·ªáu] kcal | Carb: [S·ªë li·ªáu]g
   - G·ª£i √Ω: [C√°ch ƒÉn nhanh g·ªçn]

2. ...

# V√ç D·ª§ MINH H·ªåA (EXAMPLES)
- D·ªØ li·ªáu g·ªëc: "G·∫°o t·∫ª s·ªëng" 
-> Output: 
"1. **C∆°m tr·∫Øng (N·∫•u ch√≠n)**
    - üìä Dinh d∆∞·ª°ng: 347 kcal | Carb: 75.7g
    - üîç Minh ch·ª©ng: D·ªØ li·ªáu g·ªëc l√† *'G·∫°o, tr·∫Øng, t·∫ª, s·ªëng'*
    - üí° G·ª£i √Ω: ƒÇn 1 b√°t c∆°m nh·ªè v·ªõi th·ª©c ƒÉn."
"""
class ChatRequest(BaseModel):
    question: str

@router.post("/chat")
async def chat_v2(request: ChatRequest):
    """
    API V2 Hybrid Search (Semantic + Keyword) + Cache
    """
    try:
        # 1. T·∫°o Vector cho c√¢u h·ªèi (C·∫£ 2 lo·∫°i)
        query_dense = embedder.embed_dense(request.question)
        query_sparse = embedder.embed_sparse(request.question)

        # --- B∆Ø·ªöC KI·ªÇM TRA CACHE ---
        # V·ªõi cache, ta t·∫°m th·ªùi ch·ªâ d√πng Dense Vector ƒë·ªÉ so s√°nh ƒë·ªô t∆∞∆°ng ƒë·ªìng nhanh
        cached_answer = cache_service.check_cache(query_dense)
        
        if cached_answer:
            emb_model_name = getattr(embedder, 'model_name', 'unknown-model')
            return {
                "answer": cached_answer,
                "backend_llm": "semantic_cache",
                "backend_embedding": emb_model_name,
                "context_used": ["D·ªØ li·ªáu l·∫•y t·ª´ Cache."]
            }
        # ---------------------------

        # 2. [M·ªöI] HYBRID SEARCH (T√¨m ki·∫øm lai)
        # Thay v√¨ .search(), ta d√πng .query_points() m·∫°nh h∆°n
        search_result = qdrant_client.query_points(
            collection_name=COLLECTION_NAME_V2,
            prefetch=[
                # Truy v·∫•n 1: T√¨m b·∫±ng Ng·ªØ nghƒ©a (Dense) - Hi·ªÉu √Ω ƒë·ªãnh
                models.Prefetch(
                    query=query_dense,
                    using="dense",
                    limit=100, 
                ),
                # Truy v·∫•n 2: T√¨m b·∫±ng T·ª´ kh√≥a (Sparse) - B·∫Øt ch√≠nh x√°c t√™n m√≥n
                models.Prefetch(
                    query=query_sparse.as_object(),
                    using="sparse",
                    limit=100,
                ),
            ],
            # Tr·ªôn k·∫øt qu·∫£ b·∫±ng thu·∫≠t to√°n RRF (Reciprocal Rank Fusion)
            # RRF gi√∫p c√¢n b·∫±ng: m√≥n n√†o v·ª´a ƒë√∫ng √Ω nghƒ©a, v·ª´a ƒë√∫ng t·ª´ kh√≥a s·∫Ω l√™n ƒë·∫ßu
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=30
        )

        # 3. X·ª≠ l√Ω k·∫øt qu·∫£
        if not search_result.points:
            return {
                "answer": "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin v·ªÅ m√≥n n√†y trong d·ªØ li·ªáu.",
                "backend_llm": llm_service.backend,
                "context_used": []
            }

        context_list = [hit.payload['content'] for hit in search_result.points]
        context = "\n".join(context_list)
        # --- [B∆Ø·ªöC 2] S·ª¨A PH·∫¶N T·∫†O PROMPT ---
        # Gh√©p System Prompt 
        final_prompt = f"""
        {HARDCORE_SYSTEM_PROMPT}
        
        ==============
        CONTEXT D·ªÆ LI·ªÜU (NGUY√äN LI·ªÜU TH√î):
        {context}
        ==============
        
        C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG: "{request.question}"
        
        H√ÉY TR·∫¢ L·ªúI (TU√ÇN TH·ª¶ STRICT RULES):
        """
        
        answer = llm_service.generate_answer(final_prompt)
        
        # 5. L∆∞u Cache
        cache_service.save_to_cache(query_dense, request.question, answer)
        
        emb_model_name = getattr(embedder, 'model_name', 'unknown-model')

        return {
            "answer": answer,
            "backend_llm": llm_service.backend,
            "backend_embedding": emb_model_name,
            "context_used": context_list
        }

    except Exception as e:
        # In l·ªói ra console ƒë·ªÉ debug d·ªÖ h∆°n
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))