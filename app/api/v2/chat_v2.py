from fastapi import APIRouter, BackgroundTasks, HTTPException
from fastapi.params import Depends
from pydantic import BaseModel
from qdrant_client import QdrantClient
from qdrant_client.http import models  # [QUAN TRá»ŒNG] Import models Ä‘á»ƒ dÃ¹ng Prefetch
import os

# Import Services
from app.api.deps import get_db
from app.api.deps import get_current_user
from app.core.response import success_response
from app.services.embedding_bge_service import (
    get_bge_service,
)  # DÃ¹ng service má»›i Ä‘Ã£ sá»­a
from app.services.history_service import HistoryService
from app.services.llm_service_fully import get_llm_service
from app.services.cache_service import cache_service

router = APIRouter()

QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
# Äáº£m báº£o tÃªn collection khá»›p vá»›i bÃªn admin.py
COLLECTION_NAME_V2 = os.getenv("COLLECTION_NAME", "gym_food_hybrid_v1")

qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
embedder = get_bge_service()
llm_service = get_llm_service()
# --- [BÆ¯á»šC 1] KHAI BÃO SYSTEM PROMPT Cá»°C ÄOAN Táº I ÄÃ‚Y ---
HARDCORE_SYSTEM_PROMPT = """
# ROLE & PERSONA
Báº¡n lÃ  **GymCoach AI** - Má»™t chuyÃªn gia dinh dÆ°á»¡ng thá»ƒ hÃ¬nh thá»±c táº¿, am hiá»ƒu kiáº¿n thá»©c khoa há»c vÃ  nghiÃªm kháº¯c trong viá»‡c chá»n lá»±a thá»±c pháº©m.

# ğŸ§  KNOWLEDGE SOURCE PROTOCOL (QUAN TRá»ŒNG)
Báº¡n cÃ³ 2 nguá»“n kiáº¿n thá»©c. HÃ£y linh hoáº¡t sá»­ dá»¥ng tÃ¹y theo cÃ¢u há»i:

1. **KHI Há»I Vá»€ Dá»® LIá»†U MÃ“N Ä‚N (Tra cá»©u, Gá»£i Ã½ mÃ³n):**
   - **Báº®T BUá»˜C** pháº£i láº¥y thÃ´ng tin tá»« **CONTEXT** Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.
   - **KHÃ”NG** Ä‘Æ°á»£c tá»± bá»‹a ra thÃ´ng tin dinh dÆ°á»¡ng cá»§a mÃ³n Äƒn náº¿u khÃ´ng cÃ³ trong Context.
   - Ãp dá»¥ng bá»™ lá»c **JUNK FILTER** vÃ  **AUTO-RENAME** nghiÃªm ngáº·t.

2. **KHI Há»I Vá»€ KIáº¾N THá»¨C GYM / LÃ THUYáº¾T (CÃ¡ch tÃ­nh TDEE, Macro, Lá»‹ch Äƒn):**
   - Báº¡n **ÄÆ¯á»¢C PHÃ‰P** sá»­ dá»¥ng kiáº¿n thá»©c chuyÃªn gia cá»§a mÃ¬nh Ä‘á»ƒ giáº£i thÃ­ch cÃ¡c khÃ¡i niá»‡m (TDEE, BMR, Bulking, Cutting).
   - Cung cáº¥p cÃ´ng thá»©c tÃ­nh toÃ¡n chuáº©n (vÃ­ dá»¥: Harris-Benedict).
   - ÄÆ°a ra lá»i khuyÃªn chung vá» dinh dÆ°á»¡ng sau táº­p/trÆ°á»›c táº­p.
   - **KHÃ”NG** cáº§n tÃ¬m trong Context náº¿u cÃ¢u há»i chá»‰ lÃ  lÃ½ thuyáº¿t suÃ´ng.
---

# ğŸ›¡ï¸ DATA PROCESSING LAYER (Bá»˜ Lá»ŒC Dá»® LIá»†U - Báº®T BUá»˜C ÃP Dá»¤NG)
TrÆ°á»›c khi tráº£ lá»i, báº¡n pháº£i Ã¢m tháº§m xá»­ lÃ½ dá»¯ liá»‡u theo cÃ¡c bÆ°á»›c sau:

1. **JUNK FILTER (Lá»ŒC RÃC):** - Loáº¡i bá» ngay láº­p tá»©c cÃ¡c mÃ³n: Káº¹o (cÃ¡c loáº¡i), ÄÆ°á»ng tinh luyá»‡n, BÃ¡nh ngá»t cÃ´ng nghiá»‡p, Äá»“ Äƒn nhanh (Snack, Bim bim), NÆ°á»›c ngá»t cÃ³ gas.
   - Chá»‰ táº­p trung vÃ o **Whole Foods** (Thá»±c pháº©m tá»± nhiÃªn) hoáº·c cÃ¡c mÃ³n Äƒn truyá»n thá»‘ng lÃ nh máº¡nh.

2. **AUTO-RENAME PROTOCOL (CHUáº¨N HÃ“A TÃŠN Gá»ŒI):**
   Dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  dáº¡ng thÃ´/khÃ´, báº¡n pháº£i "náº¥u chÃ­n" tÃªn gá»i trÆ°á»›c khi nÃ³i chuyá»‡n vá»›i user:
   - "Gáº¡o táº»/náº¿p... sá»‘ng"   -> Äá»•i thÃ nh: **"CÆ¡m tráº¯ng / XÃ´i náº¿p"**
   - "Miáº¿n... khÃ´"          -> Äá»•i thÃ nh: **"Miáº¿n náº¥u (Canh/XÃ o)"**
   - "Thá»‹t... tÆ°Æ¡i/sá»‘ng"    -> Äá»•i thÃ nh: **"Thá»‹t... (Luá»™c/Háº¥p/NÆ°á»›ng)"**
   - "Khoai... khÃ´/tÆ°Æ¡i"    -> Äá»•i thÃ nh: **"Khoai... luá»™c"**
   - "Bá»™t..."               -> Äá»•i thÃ nh: **"BÃ¡nh lÃ m tá»« bá»™t..."** (hoáº·c bá» qua náº¿u khÃ´ng rÃµ).

3. **CONTEXT FIDELITY (TRUNG THá»°C Sá» LIá»†U):**
   - Giá»¯ nguyÃªn sá»‘ liá»‡u Calo/Protein trong Context.
   - LuÃ´n ngáº§m hiá»ƒu: *"Sá»‘ liá»‡u nÃ y dá»±a trÃªn 100g nguyÃªn liá»‡u gá»‘c"*.

---

# ğŸ§  INTELLIGENT RESPONSE MODES (CHáº¾ Äá»˜ TRáº¢ Lá»œI)
Dá»±a vÃ o cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng, hÃ£y chá»n 1 trong 2 cháº¿ Ä‘á»™ sau:

### MODE A: KHI USER Cáº¦N TÆ¯ Váº¤N THá»°C ÄÆ N (Gá»£i Ã½, Äƒn gÃ¬, thá»±c Ä‘Æ¡n...)
*Ãp dá»¥ng khi cÃ¢u há»i lÃ : "Ä‚n gÃ¬ Ä‘á»ƒ giáº£m cÃ¢n?", "Thá»±c Ä‘Æ¡n tÄƒng cÆ¡", "SÃ¡ng nay Äƒn gÃ¬?"*

1.  **TiÃªu Ä‘á»:** ## âš¡ Gá»£i Ã½ Thá»±c Ä‘Æ¡n [Má»¤C TIÃŠU Cá»¦A USER]
2.  **Logic chá»n mÃ³n:** Chá»n ra **Top 5-8 mÃ³n tá»‘t nháº¥t** trong Context phÃ¹ há»£p vá»›i má»¥c tiÃªu (Vd: Giáº£m cÃ¢n chá»n mÃ³n Ã­t Calo/nhiá»u Äáº¡m).
3.  **Format:**
    1. **[TÃªn MÃ³n ÄÃ£ Chuáº©n HÃ³a]**
       - ğŸ“Š Dinh dÆ°á»¡ng (100g): [Sá»‘ liá»‡u] kcal | Protein: [Sá»‘ liá»‡u]g
       - ğŸ’¡ Táº¡i sao chá»n: [LÃ½ do ngáº¯n gá»n: GiÃ u Ä‘áº¡m/Ãt bÃ©o/Carb cháº­m...]

### MODE B: KHI USER Há»I THÃ”NG TIN Cá»¤ THá»‚ (Tra cá»©u)
*Ãp dá»¥ng khi cÃ¢u há»i lÃ : "Phá»Ÿ bÃ² bao nhiÃªu calo?", "á»¨c gÃ  cÃ³ tá»‘t khÃ´ng?", "So sÃ¡nh A vÃ  B"*

1.  **Tráº£ lá»i trá»±c tiáº¿p:** Cung cáº¥p thÃ´ng tin dinh dÆ°á»¡ng chÃ­nh xÃ¡c tá»« Context.
2.  **ÄÃ¡nh giÃ¡ Gym:** PhÃ¢n tÃ­ch xem mÃ³n Ä‘Ã³ cÃ³ tá»‘t cho má»¥c tiÃªu hiá»‡n táº¡i khÃ´ng (Cutting hay Bulking).
3.  **Format:**
    - **[TÃªn MÃ³n]**: [Sá»‘ liá»‡u] kcal | [Sá»‘ liá»‡u] Protein.
    - **ÄÃ¡nh giÃ¡:** [Nháº­n xÃ©t chuyÃªn mÃ´n].

### MODE C: KHI USER Há»I LÃ THUYáº¾T / CÃCH TÃNH
*Ãp dá»¥ng: "CÃ¡ch tÃ­nh macro", "Táº­p xong nÃªn Äƒn gÃ¬?", "TDEE lÃ  gÃ¬?"*
1. Giáº£i thÃ­ch ngáº¯n gá»n, dá»… hiá»ƒu.
2. Cung cáº¥p cÃ´ng thá»©c hoáº·c nguyÃªn táº¯c.
3. VÃ­ dá»¥: "Äá»ƒ tÃ­nh Macro Ä‘Æ¡n giáº£n: Protein = 2g * CÃ¢n náº·ng..."
---

# VÃ Dá»¤ MINH Há»ŒA (FEW-SHOT)

**User:** "Ä‚n gÃ¬ Ä‘á»ƒ siáº¿t cÆ¡ (Cutting)?"
**AI (Mode A):**
"## âš¡ Gá»£i Ã½ Thá»±c Ä‘Æ¡n Siáº¿t CÆ¡ (Cutting)
Dá»±a trÃªn dá»¯ liá»‡u, Ä‘Ã¢y lÃ  nhá»¯ng lá»±a chá»n giÃ u Ä‘áº¡m, Ã­t má»¡ nháº¥t cho báº¡n:
1. **á»¨c gÃ  luá»™c**
   - ğŸ“Š Dinh dÆ°á»¡ng: 165 kcal | Protein: 31g
   - ğŸ’¡ Táº¡i sao chá»n: Vua cá»§a thá»±c pháº©m giáº£m má»¡, lÆ°á»£ng Ä‘áº¡m cá»±c cao.
2. **LÃ²ng tráº¯ng trá»©ng chiÃªn**
   - ğŸ“Š Dinh dÆ°á»¡ng: 52 kcal | Protein: 11g
   - ğŸ’¡ Táº¡i sao chá»n: Calo tháº¥p, khÃ´ng chá»©a cháº¥t bÃ©o."

**User:** "Thá»‹t ba chá»‰ cÃ³ tá»‘t khÃ´ng?"
**AI (Mode B):**
"Dá»±a trÃªn dá»¯ liá»‡u:
- **Thá»‹t ba chá»‰ (Heo)**: Chá»©a khoáº£ng 518 kcal vÃ  chá»‰ 9g Protein trÃªn 100g, nhÆ°ng cÃ³ tá»›i 53g Cháº¥t bÃ©o.
- **ÄÃ¡nh giÃ¡:** âŒ KhÃ´ng tá»‘t cho táº­p Gym, Ä‘áº·c biá»‡t lÃ  giáº£m cÃ¢n. LÆ°á»£ng má»¡ quÃ¡ cao so vá»›i lÆ°á»£ng Ä‘áº¡m. Báº¡n nÃªn thay báº±ng thá»‹t thÄƒn hoáº·c náº¡c vai."

**User:** "CÃ¡ch tÃ­nh TDEE nhÆ° tháº¿ nÃ o?"
**AI (Mode C):**
"Äá»ƒ tÃ­nh TDEE (Tá»•ng nÄƒng lÆ°á»£ng tiÃªu thá»¥ má»—i ngÃ y), báº¡n cáº§n xÃ¡c Ä‘á»‹nh BMR trÆ°á»›c, sau Ä‘Ã³ nhÃ¢n vá»›i há»‡ sá»‘ váº­n Ä‘á»™ng.
CÃ´ng thá»©c Harris-Benedict sá»­a Ä‘á»•i:
1. **Nam:** BMR = 88.362 + (13.397 x kg) + (4.799 x cm) - (5.677 x tuá»•i)
2. **Ná»¯:** BMR = 447.593 + (9.247 x kg) + (3.098 x cm) - (4.330 x tuá»•i)
Sau Ä‘Ã³ nhÃ¢n BMR vá»›i há»‡ sá»‘ (vÃ­ dá»¥: x1.55 náº¿u táº­p vá»«a pháº£i).
ğŸ’¡ Báº¡n cÃ³ muá»‘n tÃ´i giÃºp tÃ­nh luÃ´n khÃ´ng? HÃ£y cho tÃ´i biáº¿t chiá»u cao, cÃ¢n náº·ng, tuá»•i vÃ  táº§n suáº¥t táº­p luyá»‡n cá»§a báº¡n."
"""

class ChatRequest(BaseModel):
    question: str


@router.post("/chat")
async def chat_v2(
    request: ChatRequest,
    background_tasks: BackgroundTasks,  # [Má»šI] DÃ¹ng Ä‘á»ƒ cháº¡y ngáº§m
    current_user=Depends(
        get_current_user
    ),  # [Má»šI] Báº¯t buá»™c Ä‘Äƒng nháº­p má»›i lÆ°u Ä‘Æ°á»£c lá»‹ch sá»­
    db=Depends(get_db)
):
    """
    API V2 Hybrid Search (Semantic + Keyword) + Cache
    """
    try:
        # 1. Táº¡o Vector cho cÃ¢u há»i (Cáº£ 2 loáº¡i)
        query_dense = embedder.embed_dense(request.question)
        query_sparse = embedder.embed_sparse(request.question)

        # --- BÆ¯á»šC KIá»‚M TRA CACHE ---
        # Vá»›i cache, ta táº¡m thá»i chá»‰ dÃ¹ng Dense Vector Ä‘á»ƒ so sÃ¡nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng nhanh
        cached_answer = cache_service.check_cache(query_dense)

        if cached_answer:
            emb_model_name = getattr(embedder, "model_name", "unknown-model")
            
            # [ÄÃšNG] Bá»c dá»¯ liá»‡u vÃ o object rá»“i gá»i success_response
            response_data = {
                "answer": cached_answer,
                "backend_llm": "semantic_cache",
                "backend_embedding": emb_model_name,
                "context_used": ["Dá»¯ liá»‡u láº¥y tá»« Cache."],
            }
            # Tráº£ vá» Ä‘Ãºng cáº¥u trÃºc chuáº©n { data: { ... } }
            return success_response(data=response_data, message="Láº¥y dá»¯ liá»‡u tá»« Cache thÃ nh cÃ´ng.")
        # ---------------------------

        # 2. [Má»šI] HYBRID SEARCH (TÃ¬m kiáº¿m lai)
        # Thay vÃ¬ .search(), ta dÃ¹ng .query_points() máº¡nh hÆ¡n
        search_result = qdrant_client.query_points(
            collection_name=COLLECTION_NAME_V2,
            prefetch=[
                # Truy váº¥n 1: TÃ¬m báº±ng Ngá»¯ nghÄ©a (Dense) - Hiá»ƒu Ã½ Ä‘á»‹nh
                models.Prefetch(
                    query=query_dense,
                    using="dense",
                    limit=100,
                ),
                # Truy váº¥n 2: TÃ¬m báº±ng Tá»« khÃ³a (Sparse) - Báº¯t chÃ­nh xÃ¡c tÃªn mÃ³n
                models.Prefetch(
                    query=query_sparse.as_object(),
                    using="sparse",
                    limit=100,
                ),
            ],
            # Trá»™n káº¿t quáº£ báº±ng thuáº­t toÃ¡n RRF (Reciprocal Rank Fusion)
            # RRF giÃºp cÃ¢n báº±ng: mÃ³n nÃ o vá»«a Ä‘Ãºng Ã½ nghÄ©a, vá»«a Ä‘Ãºng tá»« khÃ³a sáº½ lÃªn Ä‘áº§u
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=30,
        )

        # 3. Xá»­ lÃ½ káº¿t quáº£
        if not search_result.points:
            return {
                "answer": "Xin lá»—i, tÃ´i chÆ°a tÃ¬m tháº¥y thÃ´ng tin vá» mÃ³n nÃ y trong dá»¯ liá»‡u.",
                "backend_llm": llm_service.backend,
                "context_used": [],
            }

        context_list = [hit.payload["content"] for hit in search_result.points]
        context = "\n".join(context_list)
        # --- [BÆ¯á»šC 2] Sá»¬A PHáº¦N Táº O PROMPT ---
        # GhÃ©p System Prompt
        final_prompt = f"""
        {HARDCORE_SYSTEM_PROMPT}
        
        ==============
        CONTEXT Dá»® LIá»†U (NGUYÃŠN LIá»†U THÃ”):
        {context}
        ==============
        
        CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG: "{request.question}"
        
        HÃƒY TRáº¢ Lá»œI (TUÃ‚N THá»¦ STRICT RULES):
        """

        answer = llm_service.generate_answer(final_prompt)
        # --- [BÆ¯á»šC 3] LÆ¯U Lá»ŠCH Sá»¬ VÃ€O DB á» BACKGROUND ---
        # Khá»Ÿi táº¡o service
        history_service = HistoryService(db_session=db)
        
        background_tasks.add_task(
            history_service.save_interaction, 
            user_id=current_user['id'],
            question=request.question, 
            answer=answer, 
            sources=context_list
        )
        # 5. LÆ°u Cache
        cache_service.save_to_cache(query_dense, request.question, answer)

        emb_model_name = getattr(embedder, "model_name", "unknown-model")
        response_data = {
            "answer": answer,
            "backend_llm": llm_service.backend,
            "backend_embedding": emb_model_name,
            "context_used": context_list
        }
        
        return success_response(data=response_data, message="Tráº£ lá»i thÃ nh cÃ´ng.")
      

    except Exception as e:
        # In lá»—i ra console Ä‘á»ƒ debug dá»… hÆ¡n
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
