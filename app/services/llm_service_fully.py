import os
from openai import OpenAI
import requests
import google.generativeai as genai
from app.core.config import settings
from dotenv import load_dotenv

load_dotenv()

class LLMService:
    """
    Unified Service: Há»— trá»£ cáº£ code cÅ© (V1) vÃ  kháº£ nÄƒng má»Ÿ rá»™ng sang Ollama (V2).
    Tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i Backend dá»±a trÃªn file .env
    """
    def __init__(self):
        # 1. Load cáº¥u hÃ¬nh Backend
        self.backend = os.getenv("LLM_BACKEND", "gemini").lower()
        
        # 2. Cáº¥u hÃ¬nh Gemini (LuÃ´n load Ä‘á»ƒ dÃ¹ng cho Embedding cÅ© hoáº·c backup)
        try:
            # DÃ¹ng settings hoáº·c os.getenv Ä‘á»u Ä‘Æ°á»£c, Æ°u tiÃªn os.getenv cho linh hoáº¡t Docker
            api_key = os.getenv("GOOGLE_API_KEY") or settings.GOOGLE_API_KEY
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
                self.embedding_model = 'models/text-embedding-004'
        except Exception as e:
            print(f"âš ï¸ [LLM Service] Cáº£nh bÃ¡o cáº¥u hÃ¬nh Gemini: {e}")

        # 3. Cáº¥u hÃ¬nh Ollama (Quan trá»ng cho Docker)
        # LÆ°u Ã½: Trong Docker, URL nÃ y thÆ°á»ng lÃ  http://ollama:11434
        self.ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.ollama_model = os.getenv("OLLAMA_MODEL", "llama3.1")

        print(f"âš™ï¸ [LLM Service] Backend Ä‘ang cháº¡y: {self.backend.upper()}")
        if self.backend == "ollama":
            print(f"   â•°â”€ Model: {self.ollama_model} @ {self.ollama_url}")

        if self.backend == "openai":
            api_key = os.getenv("OPENAI_API_KEY") or settings.OPENAI_API_KEY
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                self.openai_model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
                print(f"ðŸ¤– [LLM Service] Backend: OPENAI ({self.openai_model})")
            else:
                print("âš ï¸ Thiáº¿u OPENAI_API_KEY!")
    # --- METHOD 1: DÃ€NH CHO API V2 (FIX Lá»–I Cá»¦A Báº N) ---
    def generate_answer(self, prompt: str) -> str:
        """
        HÃ m Ä‘Æ¡n giáº£n nháº­n vÃ o 1 prompt lá»›n (Ä‘Ã£ bao gá»“m context) vÃ  tráº£ vá» text.
        DÃ¹ng cho API V2.
        """
        if self.backend == "ollama":
            return self._call_ollama(prompt)
        elif self.backend == "openai":
            return self._call_openai(prompt)
        else:
            return self._call_gemini(prompt)

    # --- METHOD 2: DÃ€NH CHO API V1 (LEGACY) ---
    def generate_response(self, system_prompt: str, user_question: str, context: str) -> str:
        """
        HÃ m cÅ© nháº­n 3 tham sá»‘ rá»i ráº¡c. 
        Giá»¯ láº¡i Ä‘á»ƒ khÃ´ng lÃ m há»ng code cÅ©.
        """
        full_prompt = f"""
        {system_prompt}
        
        CONTEXT INFORMATION:
        {context}
        
        USER QUESTION:
        {user_question}
        """
        # TÃ¡i sá»­ dá»¥ng hÃ m generate_answer á»Ÿ trÃªn
        return self.generate_answer(full_prompt)

    # --- METHOD 3: EMBEDDING CÅ¨ (LEGACY) ---
    def get_embedding(self, text: str) -> list:
        try:
            clean_text = text.replace("\n", " ")
            result = genai.embed_content(
                model=self.embedding_model,
                content=clean_text,
                task_type="retrieval_document"
            )
            return result['embedding']
        except Exception as e:
            print(f"âŒ Lá»—i Embedding (Gemini Legacy): {e}")
            return []

    # --- INTERNAL WORKERS ---
    def _call_gemini(self, prompt: str) -> str:
        try:
            if not hasattr(self, 'gemini_model'):
                return "Lá»—i: ChÆ°a cáº¥u hÃ¬nh API Key cho Gemini."
            response = self.gemini_model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Lá»—i Gemini API: {str(e)}"
    def _call_openai(self, prompt: str) -> str:
        try:
            if not hasattr(self, 'openai_client'):
                return "Lá»—i: ChÆ°a cáº¥u hÃ¬nh OpenAI Key."
            
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    # Sá»­a á»Ÿ Ä‘Ã¢y: System prompt chung chung hÆ¡n Ä‘á»ƒ khÃ´ng override logic á»Ÿ trÃªn
                    {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ AI tuÃ¢n thá»§ tuyá»‡t Ä‘á»‘i cÃ¡c hÆ°á»›ng dáº«n trong prompt cá»§a ngÆ°á»i dÃ¹ng."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5 # Giáº£m nhiá»‡t Ä‘á»™ xuá»‘ng Ä‘á»ƒ AI bá»›t sÃ¡ng táº¡o linh tinh
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Lá»—i OpenAI API: {str(e)}"
    def _call_ollama(self, prompt: str) -> str:
        try:
            payload = {
                "model": self.ollama_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "num_ctx": 4096 
                }
            }
            response = requests.post(f"{self.ollama_url}/api/generate", json=payload, timeout=60)
            
            if response.status_code == 200:
                return response.json().get("response", "")
            else:
                return f"Lá»—i Ollama ({response.status_code}): {response.text}"
        except Exception as e:
            return f"KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c Ollama táº¡i {self.ollama_url}: {str(e)}"

# --- SINGLETON ACCESSOR ---
_llm_instance = None

def get_llm_service():
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = LLMService()
    return _llm_instance

# Legacy instance export
llm_service = get_llm_service()