import os
from openai import OpenAI
import requests
import google.generativeai as genai
from app.core.config import settings
from dotenv import load_dotenv

load_dotenv()

class LLMService:
    """
    Unified Service: H·ªó tr·ª£ c·∫£ code c≈© (V1) v√† kh·∫£ nƒÉng m·ªü r·ªông sang Ollama (V2).
    T·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi Backend d·ª±a tr√™n file .env
    """
    def __init__(self):
        # 1. Load c·∫•u h√¨nh Backend
        self.backend = os.getenv("LLM_BACKEND", "gemini").lower()
        
        # 2. C·∫•u h√¨nh Gemini (Lu√¥n load ƒë·ªÉ d√πng cho Embedding c≈© ho·∫∑c backup)
        try:
            # D√πng settings ho·∫∑c os.getenv ƒë·ªÅu ƒë∆∞·ª£c, ∆∞u ti√™n os.getenv cho linh ho·∫°t Docker
            api_key = os.getenv("GOOGLE_API_KEY") or settings.GOOGLE_API_KEY
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
                self.embedding_model = 'models/text-embedding-004'
        except Exception as e:
            print(f"‚ö†Ô∏è [LLM Service] C·∫£nh b√°o c·∫•u h√¨nh Gemini: {e}")

        # 3. C·∫•u h√¨nh Ollama (Quan tr·ªçng cho Docker)
        # L∆∞u √Ω: Trong Docker, URL n√†y th∆∞·ªùng l√† http://ollama:11434
        self.ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.ollama_model = os.getenv("OLLAMA_MODEL", "llama3.1")

        print(f"‚öôÔ∏è [LLM Service] Backend ƒëang ch·∫°y: {self.backend.upper()}")
        if self.backend == "ollama":
            print(f"   ‚ï∞‚îÄ Model: {self.ollama_model} @ {self.ollama_url}")

        if self.backend == "openai":
            api_key = os.getenv("OPENAI_API_KEY") or settings.OPENAI_API_KEY
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                self.openai_model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
                print(f"ü§ñ [LLM Service] Backend: OPENAI ({self.openai_model})")
            else:
                print("‚ö†Ô∏è Thi·∫øu OPENAI_API_KEY!")
    # --- METHOD 1: D√ÄNH CHO API V2 (FIX L·ªñI C·ª¶A B·∫†N) ---
    def generate_answer(self, prompt: str) -> str:
        """
        H√†m ƒë∆°n gi·∫£n nh·∫≠n v√†o 1 prompt l·ªõn (ƒë√£ bao g·ªìm context) v√† tr·∫£ v·ªÅ text.
        D√πng cho API V2.
        """
        if self.backend == "ollama":
            return self._call_ollama(prompt)
        elif self.backend == "openai":
            return self._call_openai(prompt)
        else:
            return self._call_gemini(prompt)

    # --- METHOD 2: D√ÄNH CHO API V1 (LEGACY) ---
    def generate_response(self, system_prompt: str, user_question: str, context: str) -> str:
        """
        H√†m c≈© nh·∫≠n 3 tham s·ªë r·ªùi r·∫°c. 
        Gi·ªØ l·∫°i ƒë·ªÉ kh√¥ng l√†m h·ªèng code c≈©.
        """
        full_prompt = f"""
        {system_prompt}
        
        CONTEXT INFORMATION:
        {context}
        
        USER QUESTION:
        {user_question}
        """
        # T√°i s·ª≠ d·ª•ng h√†m generate_answer ·ªü tr√™n
        return self.generate_answer(full_prompt)

    # --- METHOD 3: EMBEDDING C≈® (LEGACY) ---
    def get_embedding(self, text: str) -> list:
        try:
            clean_text = text.replace("\n", " ")
            result = genai.embed_content(
                model=self.embedding_model,
                content=clean_text,
                task_type="retrieval_document"
            )
            return result['embedding']
        except Exception as e:
            print(f"‚ùå L·ªói Embedding (Gemini Legacy): {e}")
            return []

    # --- INTERNAL WORKERS ---
    def _call_gemini(self, prompt: str) -> str:
        # B·ªè try-except ho·∫∑c gi·ªØ try-except nh∆∞ng ph·∫£i raise l·∫°i
        try:
            if not hasattr(self, 'gemini_model'):
                raise ValueError("Ch∆∞a c·∫•u h√¨nh API Key cho Gemini.")
            
            response = self.gemini_model.generate_content(prompt)
            
            # Ki·ªÉm tra n·∫øu response b·ªã ch·∫∑n (safety filter)
            if not response.text:
                 raise ValueError("Gemini t·ª´ ch·ªëi tr·∫£ l·ªùi (Safety Filter).")
                 
            return response.text
            
        except Exception as e:
            print(f"‚ùå Gemini Error: {e}")
            # [QUAN TR·ªåNG] N√©m l·ªói ra ngo√†i ƒë·ªÉ Controller bi·∫øt m√† d·ª´ng l·∫°i
            raise e
    def _call_openai(self, prompt: str) -> str:
        try:
            if not hasattr(self, 'openai_client'):
                return "L·ªói: Ch∆∞a c·∫•u h√¨nh OpenAI Key."
            
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    # S·ª≠a ·ªü ƒë√¢y: System prompt chung chung h∆°n ƒë·ªÉ kh√¥ng override logic ·ªü tr√™n
                    {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω AI tu√¢n th·ªß tuy·ªát ƒë·ªëi c√°c h∆∞·ªõng d·∫´n trong prompt c·ªßa ng∆∞·ªùi d√πng."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5 # Gi·∫£m nhi·ªát ƒë·ªô xu·ªëng ƒë·ªÉ AI b·ªõt s√°ng t·∫°o linh tinh
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"L·ªói OpenAI API: {str(e)}"
    def _call_ollama(self, prompt: str) -> str:
        try:
            payload = {
                "model": self.ollama_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "num_ctx": 4096 
                }
            }
            response = requests.post(f"{self.ollama_url}/api/generate", json=payload, timeout=60)
            
            if response.status_code == 200:
                return response.json().get("response", "")
            else:
                raise Exception(f"Ollama Error ({response.status_code}): {response.text}")
        except Exception as e:
            print(f"‚ùå Ollama Error: {e}")
            raise e # N√©m l·ªói ra ngo√†i

# --- SINGLETON ACCESSOR ---
_llm_instance = None

def get_llm_service():
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = LLMService()
    return _llm_instance

# Legacy instance export
llm_service = get_llm_service()